{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "reutersdata-scratch-classicML.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOKo6yvPihMfIy8C1vTCr/w",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sadidhasan/text-classification/blob/main/reutersdata_scratch_classicML.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c3vBVLQUBq7w"
      },
      "source": [
        "!mkdir reutersdata"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e2KZ8_ZRCVHD",
        "outputId": "73068a2a-8a19-46f6-d811-dc96ad741a33"
      },
      "source": [
        "%cd /content/reutersdata/\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/reutersdata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkA683mrCdDl",
        "outputId": "eb60f6e3-19ec-48c9-b9c2-b320cb2539b0"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/reutersdata\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h2GoOlEpCxok",
        "outputId": "7f54ec31-b8e7-49df-d23e-ed43ab7dda30"
      },
      "source": [
        "!wget http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-09-11 17:28:24--  http://kdd.ics.uci.edu/databases/reuters21578/reuters21578.tar.gz\n",
            "Resolving kdd.ics.uci.edu (kdd.ics.uci.edu)... 128.195.1.86\n",
            "Connecting to kdd.ics.uci.edu (kdd.ics.uci.edu)|128.195.1.86|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8150596 (7.8M) [application/x-gzip]\n",
            "Saving to: ‘reuters21578.tar.gz’\n",
            "\n",
            "reuters21578.tar.gz 100%[===================>]   7.77M  11.5MB/s    in 0.7s    \n",
            "\n",
            "2021-09-11 17:28:25 (11.5 MB/s) - ‘reuters21578.tar.gz’ saved [8150596/8150596]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GaqzOQjlC5rW",
        "outputId": "8a98d92a-6df9-436d-bc0e-74e88af72465"
      },
      "source": [
        "!ls"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "reuters21578.tar.gz\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkWnwPj6C7tb",
        "outputId": "f5bf25d2-0987-432a-95db-ac8d6d27235e"
      },
      "source": [
        "!tar zxvf reuters21578.tar.gz"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "README.txt\n",
            "all-exchanges-strings.lc.txt\n",
            "all-orgs-strings.lc.txt\n",
            "all-people-strings.lc.txt\n",
            "all-places-strings.lc.txt\n",
            "all-topics-strings.lc.txt\n",
            "cat-descriptions_120396.txt\n",
            "feldman-cia-worldfactbook-data.txt\n",
            "lewis.dtd\n",
            "reut2-000.sgm\n",
            "reut2-001.sgm\n",
            "reut2-002.sgm\n",
            "reut2-003.sgm\n",
            "reut2-004.sgm\n",
            "reut2-005.sgm\n",
            "reut2-006.sgm\n",
            "reut2-007.sgm\n",
            "reut2-008.sgm\n",
            "reut2-009.sgm\n",
            "reut2-010.sgm\n",
            "reut2-011.sgm\n",
            "reut2-012.sgm\n",
            "reut2-013.sgm\n",
            "reut2-014.sgm\n",
            "reut2-015.sgm\n",
            "reut2-016.sgm\n",
            "reut2-017.sgm\n",
            "reut2-018.sgm\n",
            "reut2-019.sgm\n",
            "reut2-020.sgm\n",
            "reut2-021.sgm\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yxRCAKORDGqa"
      },
      "source": [
        "import html\n",
        "import pprint\n",
        "import re\n",
        "from html.parser import HTMLParser\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "class ReutersParser(HTMLParser):\n",
        "    \"\"\"\n",
        "    ReutersParser subclasses HTMLParser and is used to open the SGML\n",
        "    files associated with the Reuters-21578 categorised test collection.\n",
        "\n",
        "    The parser is a generator and will yield a single document at a time.\n",
        "    Since the data will be chunked on parsing, it is necessary to keep \n",
        "    some internal state of when tags have been \"entered\" and \"exited\".\n",
        "    Hence the in_body, in_topics and in_topic_d boolean members.\n",
        "    \"\"\"\n",
        "    def __init__(self, encoding='latin-1'):\n",
        "        \"\"\"\n",
        "        Initialise the superclass (HTMLParser) and reset the parser.\n",
        "        Sets the encoding of the SGML files by default to latin-1.\n",
        "        \"\"\"\n",
        "        html.parser.HTMLParser.__init__(self)\n",
        "        self._reset()\n",
        "        self.encoding = encoding\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"\n",
        "        This is called only on initialisation of the parser class\n",
        "        and when a new topic-body tuple has been generated. It\n",
        "        resets all off the state so that a new tuple can be subsequently\n",
        "        generated.\n",
        "        \"\"\"\n",
        "        self.in_body = False\n",
        "        self.in_topics = False\n",
        "        self.in_topic_d = False\n",
        "        self.body = \"\"\n",
        "        self.topics = []\n",
        "        self.topic_d = \"\"\n",
        "\n",
        "    def parse(self, fd):\n",
        "        \"\"\"\n",
        "        parse accepts a file descriptor and loads the data in chunks\n",
        "        in order to minimise memory usage. It then yields new documents\n",
        "        as they are parsed.\n",
        "        \"\"\"\n",
        "        self.docs = []\n",
        "        for chunk in fd:\n",
        "            self.feed(chunk.decode(self.encoding))\n",
        "            for doc in self.docs:\n",
        "                yield doc\n",
        "            self.docs = []\n",
        "        self.close()\n",
        "\n",
        "    def handle_starttag(self, tag, attrs):\n",
        "        \"\"\"\n",
        "        This method is used to determine what to do when the parser\n",
        "        comes across a particular tag of type \"tag\". In this instance\n",
        "        we simply set the internal state booleans to True if that particular\n",
        "        tag has been found.\n",
        "        \"\"\"\n",
        "        if tag == \"reuters\":\n",
        "            pass\n",
        "        elif tag == \"body\":\n",
        "            self.in_body = True\n",
        "        elif tag == \"topics\":\n",
        "            self.in_topics = True\n",
        "        elif tag == \"d\":\n",
        "            self.in_topic_d = True \n",
        "\n",
        "    def handle_endtag(self, tag):\n",
        "        \"\"\"\n",
        "        This method is used to determine what to do when the parser\n",
        "        finishes with a particular tag of type \"tag\". \n",
        "\n",
        "        If the tag is a  tag, then we remove all \n",
        "        white-space with a regular expression and then append the \n",
        "        topic-body tuple.\n",
        "\n",
        "        If the tag is a  or  tag then we simply set\n",
        "        the internal state to False for these booleans, respectively.\n",
        "\n",
        "        If the tag is a  tag (found within a  tag), then we\n",
        "        append the particular topic to the \"topics\" list and \n",
        "        finally reset it.\n",
        "        \"\"\"\n",
        "        if tag == \"reuters\":\n",
        "            self.body = re.sub(r'\\s+', r' ', self.body)\n",
        "            self.docs.append( (self.topics, self.body) )\n",
        "            self._reset()\n",
        "        elif tag == \"body\":\n",
        "            self.in_body = False\n",
        "        elif tag == \"topics\":\n",
        "            self.in_topics = False\n",
        "        elif tag == \"d\":\n",
        "            self.in_topic_d = False\n",
        "            self.topics.append(self.topic_d)\n",
        "            self.topic_d = \"\"  \n",
        "\n",
        "    def handle_data(self, data):\n",
        "        \"\"\"\n",
        "        The data is simply appended to the appropriate member state\n",
        "        for that particular tag, up until the end closing tag appears.\n",
        "        \"\"\"\n",
        "        if self.in_body:\n",
        "            self.body += data\n",
        "        elif self.in_topic_d:\n",
        "            self.topic_d += data\n",
        "\n",
        "\n",
        "def obtain_topic_tags():\n",
        "    \"\"\"\n",
        "    Open the topic list file and import all of the topic names\n",
        "    taking care to strip the trailing \"\\n\" from each word.\n",
        "    \"\"\"\n",
        "    topics = open(\n",
        "        \"/content/reutersdata/all-topics-strings.lc.txt\", \"r\"\n",
        "    ).readlines()\n",
        "    topics = [t.strip() for t in topics]\n",
        "    return topics\n",
        "\n",
        "def filter_doc_list_through_topics(topics, docs):\n",
        "    \"\"\"\n",
        "    Reads all of the documents and creates a new list of two-tuples\n",
        "    that contain a single feature entry and the body text, instead of\n",
        "    a list of topics. It removes all geographic features and only \n",
        "    retains those documents which have at least one non-geographic\n",
        "    topic.\n",
        "    \"\"\"\n",
        "    ref_docs = []\n",
        "    for d in docs:\n",
        "        if d[0] == [] or d[0] == \"\":\n",
        "            continue\n",
        "        for t in d[0]:\n",
        "            if t in topics:\n",
        "                d_tup = (t, d[1])\n",
        "                ref_docs.append(d_tup)\n",
        "                break\n",
        "    return ref_docs\n",
        "\n",
        "def create_tfidf_training_data(docs):\n",
        "    \"\"\"\n",
        "    Creates a document corpus list (by stripping out the\n",
        "    class labels), then applies the TF-IDF transform to this\n",
        "    list. \n",
        "\n",
        "    The function returns both the class label vector (y) and \n",
        "    the corpus token/feature matrix (X).\n",
        "    \"\"\"\n",
        "    # Create the training data class labels\n",
        "    y = [d[0] for d in docs]\n",
        "    \n",
        "    # Create the document corpus list\n",
        "    corpus = [d[1] for d in docs]\n",
        "\n",
        "    # Create the TF-IDF vectoriser and transform the corpus\n",
        "    vectorizer = TfidfVectorizer(min_df=1, max_features=20000)\n",
        "    X = vectorizer.fit_transform(corpus)\n",
        "    return X, y\n",
        "\n",
        "def train_svm(X, y):\n",
        "    \"\"\"\n",
        "    Create and train the Support Vector Machine.\n",
        "    \"\"\"\n",
        "    svm = SVC(C=1000000.0, gamma='scale', kernel='rbf')\n",
        "    svm.fit(X, y)\n",
        "    return svm\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EoqhF-pKIBUu"
      },
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Create the list of Reuters data and create the parser\n",
        "    files = [\"/content/reutersdata/reut2-%03d.sgm\" % r for r in range(0, 22)]\n",
        "    parser = ReutersParser()\n",
        "\n",
        "    # Parse the document and force all generated docs into\n",
        "    # a list so that it can be printed out to the console\n",
        "    docs = []\n",
        "    for fn in files:\n",
        "        for d in parser.parse(open(fn, 'rb')):\n",
        "            docs.append(d)\n",
        "\n",
        "    # Obtain the topic tags and filter docs through it \n",
        "    topics = obtain_topic_tags()\n",
        "    ref_docs = filter_doc_list_through_topics(topics, docs)\n",
        "    \n",
        "    # Vectorise and TF-IDF transform the corpus \n",
        "    X, y = create_tfidf_training_data(ref_docs)\n",
        "\n",
        "    # Create the training-remain split of the data\n",
        "    X_train, X_rem, y_train, y_rem = train_test_split(\n",
        "        X, y, train_size=0.8, random_state=42\n",
        "    )\n",
        "\n",
        "    X_valid, X_test, y_valid, y_test = train_test_split(X_rem,y_rem, test_size=0.5)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2E53fvBXD7E_"
      },
      "source": [
        "    # Create and train the Support Vector Machine\n",
        "    svm = train_svm(X_train, y_train)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pfC0RLcCXEOZ",
        "outputId": "b310cffc-069a-405b-c0c5-98ed73d76d82"
      },
      "source": [
        "# Make an array of predictions on the test set\n",
        "pred = svm.predict(X_test)\n",
        "\n",
        "# Output the hit-rate and the confusion matrix for each model\n",
        "print(svm.score(X_test, y_test))\n",
        "print(confusion_matrix(pred, y_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.8188214599824098\n",
            "[[239   5   0 ...   1   1   0]\n",
            " [  0   4   0 ...   0   0   0]\n",
            " [  0   0   1 ...   0   0   0]\n",
            " ...\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]\n",
            " [  0   0   0 ...   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wv6JdNjW8YdI",
        "outputId": "0c357062-0f25-4794-a08a-d5391e68d673"
      },
      "source": [
        "from sklearn import metrics\n",
        "\n",
        "print(\"Accuracy of SVM:\",metrics.accuracy_score(y_test, pred))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of SVM: 0.8188214599824098\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wq6ofCFf7nNM",
        "outputId": "db13197b-2547-403a-ba05-88f5061ed766"
      },
      "source": [
        "#using a logistic regression model\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "logreg = LogisticRegression(max_iter=200)\n",
        "logreg.fit(X_train,y_train)\n",
        "y_pred = logreg.predict(X_test)\n",
        "print(\"Accuracy of logreg:\",metrics.accuracy_score(y_test, y_pred))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of logreg: 0.7985927880386984\n"
          ]
        }
      ]
    }
  ]
}